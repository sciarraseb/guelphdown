
```{r package_loading_int_conc, include=F}
#devtools::install_github(repo = 'sciarraseb/nonlinSimsAnalysis', force = T)
library(easypackages)
packages <- c('tidyverse', 'RColorBrewer', 'parallel', 'data.table', 'kableExtra', 'ggtext', 'egg', 'ggbrace', 'cowplot', 'nonlinSimsAnalysis', 'nonlinSims')
libraries(packages)

knitr::opts_chunk$set(message = F)
```


# General Discussion 

In systematically reviewing the simulation literature, I found that no study had comprehensively investigated the effects of longitudinal design and
analysis factors on the modelling accuracy of nonlinear patterns of change. Specifically, no study had investigated any possible three-way interaction between any of the following four variables: 1) measurement spacing, 2) number of measurements, 3) sample size, and 4) time structuredness. Given that longitudinal designs are necessary to understand the temporal dynamics of psychological processes (for a more detailed explanation, see Appendix \ref{ergodicity}), it is important that researchers understand how longitudinal design and analysis factors affect the accuracy of longitudinal analyses and if there are any interactions between these factors. Therefore, to address these gaps in the literature, I  designed three simulation experiments.

In each simulation experiment, a logistic pattern of change (i.e., s-shaped change pattern) was modelled under conditions that varied in nature of change, measurement number, sample size, and time structuredness.\footnote{Importantly, no simulation experiment manipulated more than three variables at once.} To fit a logistic function where each parameter could be meaningfully interpreted, each simulation experiment used a structured latent growth model to estimate nonlinear change (for a detailed explanation, see Appendix \ref{structured-lgc}). 

To investigate the effects of longitudinal design and analysis factors on modelling accuracy, my simulation experiments examined the accuracy with which each logistic function parameter was estimated. Thus, modelling accuracy was defined at the parameter level. Importantly, in giving modelling accuracy a parameter-level definition, it became important to measure two metrics: bias and precision. For any given logistic function parameter, two questions are of central importance: 1) How well is the parameter estimated on average (bias) and 2) what is a plausible range of values that can be expected for an estimate from the output of a single model (precision). Therefore, bias and precision were computed for the estimation of each logistic function parameter. To succinctly summarize each experiment, I have created Table \ref{tab:exp-summary-table}. Each row of Table \ref{tab:exp-summary-table} contains a summary of a simulation experiment.

In Experiment 1, I was interested in answering two questions: 1) Does placing measurements near periods of change increase modelling accuracy and 2) how should measurements be spaced when the nature of change is unknown. To answer these two questions, I manipulated measurement spacing, number of measurements, and nature of change. With respect to the first question, the results of Experiment 1 suggest that modelling accuracy increases when measurements are placed closer to periods fo change (see section discussing [measurement spacing](#meas-placing)). With respect to the second question, the results of Experiment 1 suggest that measurements should be spaced equally over time when the nature of change is unknown (see section discussing measurement spacing when the nature of change is [unknown](#unknown)). 

```{r exp-summary-table, echo=F}
summary_df <- data.frame('Simulation Exeriment' = c("Experiment 1", "Experiment 2", "Experiment 3"), 
                         'Independent Variables' = c("\\thead[lt]{Spacing of measurements \\\\ Number of measurements \\\\ Nature of change}", 
                                                     "\\thead[lt]{Spacing of measurements \\\\ Number of measurements \\\\ Sample size}", 
                                                     "\\thead[lt]{Number of Measurements \\\\ Sample size \\\\ Time structuredness}"), 
                         'Main Results' = c('\\tabitem Modelling accuracy is higher when measurements are placed closer to periods of change \\newline
                                            \\tabitem Measurements should be spaced equally when the nature of change is unknown',
                                            '\\tabitem The greatest improvements in modelling accuracy result from using either seven measurements with $N \\ge$ 200 or nine measurements with $N \\le$ 100',
                                            '\\tabitem The greatest improvements in modelling accuracy across all time structuredness levels result from using either seven measurements with $N \\ge$ 200 or nine measurements with $N \\le$ 100 \\newline
                                            \\tabitem Use definition variables to prevent modelling accuracy from decreasing as time structuredness decreases'), check.names = F)



kbl(x = summary_df, format = 'latex', digits = 2,align = c('l', 'l', 'l'), 
    longtable = T, booktabs = T,  escape = F,
    caption = 'Summary of Each Simulation Experiment') %>%
    column_spec(column = 3, width = '7.25cm') %>%
   kable_styling(latex_options= c('repeat_header'), position = 'left')

```

In Experiment 2, I was interested in the measurement number/sample size pairings needed to obtain accurate modelling (i.e., low bias, high precision) under different spacing schedules. To answer this question, I manipulated measurement spacing, measurement number, and sample size. Although no manipulated measurement number-sample size pairing results in high modelling accuracy (low bias, high precision) of all parameters, the largest improvements in modelling accuracy result from using moderate measurement numbers and sample sizes. For all spacing schedules (except middle-and-extreme spacing), the largest improvements in modelling accuracy result from using either either seven measurements with *N* $\ge$ 200 or nine measurements with *N* $\le$ 100.  The results for middle-and-extreme spacing are largely an effect of the nature of change used in Experiment 2, and so are of little value to emphasize. 

In Experiment 3, I was interested in examining how time structuredness affected modelling accuracy. To answer this question, I manipulated measurement spacing, measurement number, and time structuredness. Although the measurement number-sample size pairings that result in the greatest improvements in modelling accuracy are the same as in Experiment 2, two results suggest that modelling accuracy decreases as time structuredness decreases. First, precision decreases as time structuredness decreases. Second, and more concerning, bias decreases as time structuredness decreases regardless of the measurement number or sample size. Importantly, the decrease in modelling accuracy can be prevented by using a latent growth curve model with definition variables, which I showed in an additional set of simulations (see section on [definition variables](#def-variables)). Therefore, to obtain the largest improvements in modelling accuracy, either seven measurements with *N* $\ge$ 200 or nine measurements with *N* $\le$ 100 must be used and, importantly, the latent growth curve model must use definition variables. 

In summary, the results of my simulation experiments are the first (to my knowledge) to provide specific measurement number and sample size recommendations needed to accurately model nonlinear change over time. Importantly, although previous studies have investigated the effects of some longitudinal design and analysis factors on the modelling accuracy of nonlinear patterns, the results of these studies are limited because they either used unrealistic fixed-effects models [e.g., @finch2017], models with non-meaningful parameter interpretations [e.g., @fine2019; @liu2022], or unrealistic model fitting procedures [@finch2017]. Additionally, I developed novel and replicable procedures for creating spacing schedules (see Appendix \ref{measurement-schedules}) and simulating time structuredness (see [time structuredness](#simulating-time-struc)). 

The sections that follow will discuss....

## Limitations and Future Directions 

### Cutoff Values for Modelling Accuracy

In simulation research, cutoff values for parameters are often set to a percentage of a parameter's population value [e.g., @muthen1997] for two reasons. First, cutoff values are needed to allow the biasness and precision of modelling performance to be categorized so that results can be clearly presented. In the current set of simulation experiments, cutoff values for bias and precision were set to 10% of the parameter's population value [@muthen1997]. If a parameter estimate lied outside a 10%-error margin, then estimation was considered biased. If an error bar whisker length was longer than 10% of the parameter's population value, then estimation was considered imprecise. Therefore, using cutoff values allows categorical decisions to be made modelling performance. 

Second, cutoff values are needed to allow results from different simulation studies to be meaningfully compared. If another study uses a cutoff value of 15%, then the results of this comparison cannot be validly compared to the results of the current simulation experiments because each study uses different standards. Therefore, it is important that simulation studies use a common standard of 10% [@muthen1997]---as I have done in my simulation experiments. Although simulation studies use cutoff values to simplify results and allow meaningful comparisons of results, it is also important that cutoff values themselves represent meaningful boundary values.  

In simply defining cutoff values as a percentage of a population value, cutoff values can become meaningless and lead to problematic decision making. As a simple example, consider a scenario where a beverage company wants to produce a caffeinated drink that can only increase heart rate and body temperature by a certain amount. Specifically, neither heart rate nor body temperature can increase by 10% of their resting values. Given that, for males and females, any value below 70 and 80, respectively, constitutes a healthy resting heart rate [@nanchen2018], a 10% increase would translate to an increase of 7 and 8 beats per minute, which is arguably less than the increase in heart rate caused from walking [e.g., @whitley1987]. Thus, requiring that a caffeinated drink not increase resting heart rate by a value equal to or greater than 10% appears to be a responsible stipulation. Unfortunately, setting a 10%-cutoff rule for body temperature allows far less desirable outcomes than a 10% cutoff for heart rate. Using a typical body temperature of 37 $^\circ$C for resting body temperature, a 10%-cutoff would allow for a change in body temperature of 3.7 $^\circ$C. Given that deviations of less than 3.7 $^\circ$C from resting body temperature can lead to physiological impairments and even death [@moran2002], restricting the caffeinated drink to not increase body temperature by 10% of its resting value is unwise. Therefore, a percentage cutoff rule can fail to create meaningful cutoff values by overlooking the underlying nature of the corresponding variable.

In the current simulation experiments, the percentage-cutoff rule may have led to incorrect decisions about modelling performance. As an example, consider the estimation of the random effect parameters. In each simulation experiment, no measurement number/sample size pairing resulted in high modelling accuracy (low bias, high precision) of any random-effect parameter. Specifically, the random-effect day-unit parameters were never modelled precisely with any measurement number/sample size pairing.\footnote{Note that not even any of the random-effect Likert-unit parameters were ever modelled with high precision by any manipulated measurement number/sample size pairing.} Although the lack of accurate estimation for all parameters seems concerning, the result be a byproduct of how cutoff values were set to deem estimation as precise. For a given parameter, the cutoff value used to deem estimation as unbiased and precise was proportional to the population value set for that parameter. Specifically, the cutoff values for bias and precision were set to 10% of the parameter's population value [@muthen1997]. In setting the cutoff value to a percentage of the parameter's population value, the margin of error becomes a function of the population value: Large population values have large margins of error and small population values have small margins of error. Given that the random-effect parameters had the smallest population values (e.g., 10.00, 4.00, and 0.05) and that even the largest measurement number-sample size pairing of 11 measurements with *N* = 1000 did not model with high precision, it is conceivable that the associated 10%-error margins (e.g., 1.00, 0.04, and 0.005) may have been too small. 

Because percentage cutoff rules are relative to the population value set for a parameter, they can often be arbitrary. One way to ensure less arbitrary cutoff values is to conceptualize cutoff values as smallest effect sizes of interest [@lakens2017; @lakens2018]. Introduced to improve to null-hypothesis significance testing, a smallest effect size of interest constitutes the smallest effect size above which a researcher considers an observed effect meaningful [@lakens2017]. Instead of testing the typical zero-effect null hypothesis, a researcher can specify a smallest effect size of interest as the null hypothesis. Using a smallest effect size of interest (in tandem with equivalence testing), a researcher can more definitely conclude whether an effect is trivially small or not and, consequently, be less likely to incorrectly dismiss an effect as nonexistent. Thus, smallest effect sizes of interest allow researchers to make more meaningful conclusions. Although the current simulation experiments did not employ significance testing, the cutoff values used to determine whether estimation was biased and precise could be improved by treating them as smallest cutoff values of interest. By replacing the current percentage-based cutoff values with smallest cutoff values of interest for each parameter, conclusions are likely to become more meaningful. 

One effective way to determine smallest cutoff values of interest is to use anchor-based methods [@anvari2021]. As an example, I detail a two-step procedure for how an anchor-based method could be used to determine a cutoff value for a the Likert-unit parameter of the fixed-effect baseline parameter ($\uptheta_{fixed}$). First, a survey for some Likert-unit variable such as job satisfaction could be given at two time points to employees. Importantly, after completing the survey at the second time point, employees would also indicate how much job satisfaction changed by answering an anchor question (e.g., "Job satisfaction increased/decreased by a little, increased/decreased a lot, or did not change."). Second, a smallest cutoff value of interest would need to be computed at two time points. Given that the fixed-effect baseline parameter ($\uptheta_{fixed}$) represents the starting value, then employees that indicated no change in job satisfaction could be said to still be at baseline. Thus, to compute a smallest cutoff value of interest for the fixed-effect baseline parameter ($\uptheta_{fixed}$), the mean change in job satisfaction could be computed using data from employees that indicated no change. Therefore, in using the anchor-based method, the smallest cutoff value of interest for the fixed-effect baseline parameter ($\uptheta_{fixed}$) is the mean change in some Likert-unit variable---job satisfaction in the current example---from respondents that indicated no change.\footnote{If the mean observed change in job satisfaction from employees that indicate no change is a near-zero value, using this value as a smallest effect-size of interest for the fixed-effect baseline parameter ($\uptheta_{fixed}$) would likely be too conservative. In such situations, the smallest effect-size of interest for the fixed-effect baseline parameter ($\uptheta_{fixed}$) could be determined by computing the mean change in job satisfaction from employees that indicated a small change (i.e., 'little increase/decrease), as it could be said that these employees have slightly moved away from baseline.}

In summary, percentage-based cutoff values are used in simulation research to allow categorical decisions to be made about modelling performance and to enable results to be meaningfully compared between studies. Unfortunately, in defining cutoff values as a percentage of a population value, cutoff values can become meaningless and lead to problematic decision making. To set cutoff values that specify meaningful boundaries, cutoff values can be conceptualized as smallest effect sizes of interest. In empirical research, a smallest effect size of interest constitutes the smallest effect size above which a researcher considers an observed effect meaningful [@lakens2017]. By using smallest effect sizes of interest, a researcher can more definitely conclude whether an effect is trivially small or not and, consequently, be less likely to incorrectly dismiss an effect as nonexistent. Similar to empirical research, smallest cutoff values of interest can be used in simulation research so that so that quantitative researchers can more definitely identify conditions that produce meaningful amounts of error. To determine smallest cutoff values of interest, anchor-based methods [@anvari2021] can be used. 


### External Validity of Simulation Experiments

(ref:dillman2014) [see @dillman2014]
In the current set of simulation experiments, data were were generated under ideal conditions in three ways. First, the current simulation experiments always assumed complete data (i.e., 100% response rate). Unfortunately, researchers rarely obtain complete data and, instead, have some amount of data that are missing. One investigation estimated that, using a sample of 300 articles published over a period of three years, 90% of articles had missing data, with each study estimated to have over 30% of data points missing [@mcknight2007, Chapter 1]. Perhaps even more concerning, missing data often compound over time [@newman2003].\footnote{It should be noted that great recommendations exist on increasing response rate. In fact, an entire book of recommendations exists on this issue (ref:dillman2014).} To simulate more realistic conditions for response rates in longitudinal designs, missing data could be set to increase---either linearly or nonlinearly---over time under three types of commonly simulated missing data mechanisms: 1) missing data are random, 2) missing data depend on the value of another variable, and 3)  missing data depend on their own values [@newman2009]. 

Second, the current simulation experiments assumed measurement invariance over time. That is, at each time point, the manifest variable is measured with the same measurement model---specifically, aspects of the measurement model such as factor loadings, intercepts, and error variances remain constant over time [@mellenbergh1989; @vandenberg2000]. For a longitudinal design, it is important that the measurement of a latent variable meet the conditions for invariance so that change over time can be meaningfully interpreted. As an example, consider a situation where a researcher measures some latent variable over time such as job satisfaction using a four-item survey where each item measures some component of job satisfaction on a Likert scale (range of 1--5). If the loadings of a specific item change over time, then the response values from participants cannot be meaningfully interpreted. For example, if a participant gives the same answers to each item across two time points but factor loadings of any item(s) change between the two time points, then their job satisfaction scores between the time points will, counterintuitively, be different. Thus, even though job satisfaction did not change over time, changes in the measurement model of job satisfaction caused the observed scores to be different. Unfortunately, measurement invariance is seldom observed [@vandenberg2000; @vandeschoot2015] because measurement model components often change over time [e.g., @fried2016]. Thus, it can be argued that it is more realistic to assume measurement non-invariance. To simulate measurement non-invariance, data could be generated such that aspects of the measurement model change are set to change over time [e.g., @kim2014a]. 

Third, the current simulations assumed error variances in the observed variables to be constant and uncorrelated over time. Unfortunately, error variances over time are likely to correlate with each other and be nonconstant or heterogeneous [@goldstein1994; @deshon1998; @bliese2002; @braun2013; @ding2016; @lester2019; @blozis2018]. To simulate a more realistic error variance structure, simulations could simulate errors to correlate with each other and to decrease over time---an observation in a longitudinal analysis of fatigue [@lang2018]. 

In summary, the current simulation experiments investigated the effects of longitudinal design and analysis factors under ideal conditions. Data were generated with no missing data, measurement invariance, and simple error structures (i.e., homogeneous and uncorrelated errors). The current simulations acted as an first step for developing an initial understanding of the effects of longitudinal design and analysis factors on model performance. A necessary next step for future research is to simulate data under conditions more representative of those encountered when conducting longitudinal designs. Specifically, future simulation experiments could generate missing data to increase over time [e.g., @newman2003], measurements to be non-invariant [e.g., @kim2014a], and data to have complex error structures [e.g., @sivo2000]. Given that missing data [e.g., @newman2003, @kombo2016], measurement noninvariance [e.g., @chen2005; @kim2014a; @hsiao2018], and complex error structures [e.g.,@sivo2000; @sivo2005; @sivo2008; @grimm2010a] have been found to independently decrease model performance, it is likely that obtain adequate model performance will be more difficult under more realistic conditions. Consequently, as data are generated under increasingly realistic conditions, it is likely that more stringent measurement number/sample size pairings will be needed to result in adequate model performance.

### Simulations With Other Longitudinal Analyses

Given that researchers are often interested in investigating questions outside of modelling a nonlinear pattern of change, longitudinal analyses outside of the structured latent growth curve model used in the current simulation experiments must be used. Although the structured latent growth curve modelling framework used in the current simulations allows nonlinear change to be meaningfully modelled (see Appendix \ref{structured-lgc-code}), the framework cannot be used to understand all meaningful components of change. As an example, if a researcher is interested in heterogeneous response patterns in some variable in response to some organizational event---for instance, work engagement patterns after mergers [@seppälä2018]---a structured latent growth curve model could not meaningfully model such data because it assumes one pattern of responding. Therefore, different models must be used to develop a comprehensive understanding of change over time, and it is important that simulation research investigate how each of these models perform with different longitudinal designs. In the paragraphs that follow, I outline four longitudinal analyses that future simulation experiments should investigate. 

(ref:cudeck2002) @cudeck2002
(ref:edwards2017) [for a review, see @edwards2017]
(ref:chou2004) [@chou2004; @kohli2013]

First, discontinuous growth models are needed to model punctuated change [@bliese2016; @bliese2020].\footnote{In the multilevel framework, discontinuous growth modelling is also referred to as hierarchical linear modelling (ref:) and multiphase mixed-effects models (ref:cudeck2002). In the latent variable or structural equation modelling framework, discontinuous growth modelling is also referred to as piecewise growth modelling (ref:chou2004). Note that spline models are technically different from discontinuous growth models because spline models cannot model vertical displacements at knot points and, thus, are models for continuous change (ref:edwards2017).} Given that change in organizations often results from discrete events, the pattern of change is often punctuated or discontinuous [@morgeson2015]. Examples of punctuated change in organizations have been observed in life satisfaction after unemployment [@lucas2004], trust after betrayal [@fulmer2015], and firm performance after an economic recession [@kim2014b; for more examples, see @bliese2016]. Discontinuous growth models can model punctuated change by selectively activating and deactivating growth factors---that is, assigning nonzero- and zero-value weights, respectively---after certain time points [@bliese2016]. Therefore, given that punctuated change merits the need for discontinuous growth modelling in organizational research, future simulation studies should investigate the effects of longitudinal design and analysis factors on the performance of such models. 

(ref:meehl1978) [@meehl1978]
(ref:borsboom2004) [for a review, see @borsboom2004]

Second, time series models are needed to model cyclical patterns [@pickup2014]. Technological advances such as smartphones and wearable sensors have allowed researchers to collect intensive longitudinal data sets where data are collected over at least 20 time points [@collins2006] with the experience sampling method [@larson2014]. With intensive longitudinal data sets, researchers are often interested in modelling cyclical patterns such as those with affect and performance [@dalal2014] and stress [@fuller2003]. Time series models allow researchers to model cyclical patterns provide an effective method for modelling cyclical patterns by through a variety of methods (e.g., decomposition, autoregressive integrated moving average, etc.). Therefore, given the interest for modelling cyclical patterns with intensive longitudinal data merits the use of time series models, future simulation studies should investigate the effects of longitudinal design and analysis factors on the performance of such models. 

Third, second-order growth models are needed to model measurement invariance [@sayer2001; @hancock2001]. In organizational research, many variables are latent---that is, they cannot be directly observed (e.g., job satisfaction, organizational commitment, trust). Because latent variables cannot be directly measured, nomological networks\footnote{Although a nomological network gives meaning to a latent variable by specifying relations with other variables, it must be noted that the network is an ineffective tool for establishing validity---whether a survey measures what is purports to measure. In psychology, almost all variables psychology are correlated with each other (ref:meehl1978), and so using the correlations specified in a nomological network to establish validity is ineffective because many latent variables are likely to satisfy the network of relations. A more effective way to establish validity is to first assume the existence of the latent variable and then develop theory that specifies processes by which changes in the a latent variable manifest themselves in reality. Surveys can the be constructed by causatively testing whether the theorized manifestations that follow from changes in a latent variable actually emerge (ref:borsboom2004).}---correlation matrices specifying relations between the target latent variable and other variables---are constructed to develop valid measures of latent variables {@cronbach1955].  As discussed previously, an unfortunate phenomenon with surveys is that the accuracy with which they measure a latent variable is seldom invariant over time---that is, measurement accuracy is often non-invariant [@vandenberg2000; @vandeschoot2015]. If measurement non-invariance is overlooked, model performance decreases [@kim2014b; @jeon2020]. Fortunately, second-order latent growth curve models allow researchers to include measurement models and, thus, test for measurement invariance and estimate parameters with greater accuracy [e.g., @kim2014a]. Therefore, given that the common occurrence of measurement non-invariance in organizational research merits the use of second-order latent growth models, future simulation studies should investigate the effects of longitudinal design and analysis factors on the performance of such models. 

Fourth, growth mixture models are needed to model heterogeneous response patterns [@wang2007; @vandernest2020]. In organizations, employees are likely to respond to changes in different ways, thus exhibiting heterogeneous response patterns. Examples of heterogeneous response patterns have been observed in job performance patterns during organizational restructuring [@miraglia2015], work engagement patterns after mergers [@seppälä2018], and leadership development throughout training [@day2011]. Growth mixture models allow heterogeneity in response patterns to be modelled by including a latent categorical variable that allows participants to be placed into different response category patterns [cf. @bauer2007]. Therefore, given that heterogeneous response patterns in organizations merit the use of interest for modelling cyclical patterns with intensive longitudinal data merits the use of time series models, future simulation studies should investigate the effects of longitudinal design and analysis factors on the performance of such models. 

In summary, researchers are often interested in investigating at least four types of questions with longitudinal data and each of these questions merits the use of a different analysis. Discontinuous growth models must be used to model punctuated change [@bliese2016; @bliese2020], time series models are needed to model cyclical patterns [@pickup2014], second-order growth models are needed to model measurement invariance [@sayer2001; @hancock2001], and growth mixture models are needed to model heterogeneous response patterns [@wang2007]. Thus, future simulation research should investigate how longitudinal design and analysis factors affect the model performance each of these analyses.  


## Nonlinear Patterns and Longitudinal Research
### Redefining Longitudinal Designs

The results of the current simulation experiments suggest that previous measurement number recommendation for longitudinal research need to be modified. Previous suggestions for conducting longitudinal research recommend that at least three measurements be used [@chan1998;@ployhart2010]. The requirement that a longitudinal study use at least three measurements is largely to enable allow nonlinear change to be modelled and to obtain an estimate of change that is not confounded by measurement error [@rogosa1982]. Unfortunately, however, using at recording change over at least three time point provide no guarantee that a nonlinear pattern of change will be accurately modelled. The results of the current simulation experiments suggest that, at the very least, five measurements are needed to accurately model a nonlinear pattern of change. Importantly, five measurements only results in adequate model performance if the measurements are placed near periods of change. Given that organizational theories seldom delineate nonlinear patterns of change [for a rare example, see @methot2017], it is unlikely that researchers will place measurements near periods of change. In situations where researchers have little insight into the pattern of nonlinear change, the current simulation experiments suggest that at least seven measurements be used. 


### Why is it Important to Model Nonlinear Patterns of Change? 

For at least 30 years, research in organizational psychology has had a minimal effect on the implementations of practitioners [@daft1990; for a review, see @lawler2022]. Almost no practitioner--specifically, an estimated 1%---reads journal articles [@rynes2002a], which is accompanied by a poor understanding of fundamental organizational psychology principles in managers across multiple cultures including the Netherlands [@sanders2008], the United States [@rynes2002], Finland, South Korea, and Spain [@tenhiälä2014]. Perhaps most unfortunate, a poor understanding of organizational psychology in managers is associated with large effects on financial and individual performance [for a review, see @rynes2002b] an estimated 55% of practitioners are skeptical that evidence-based human resource practices can affect any positive change [@kpmg2015]. With the gap between academics and practitioners being so patently wide, some academics have cast doubt on the possibility of academic- practitioner research collaborations [@kieser2009].  

One factor that likely contributes to the academic-practitioner gap is the paucity of specific recommendations. Comprehensive investigations of the organizational literature have found that an estimated 3% of human resources articles address the problems of practitioners [@sackett1990] and, in a review of 5780 articles from 1963--2007, it was concluded that research is often late to address practitioner issues [@cascio2008]. The paucity of specific recommendations in organizational research becomes evident when looking at the most prominent theories: Almost all propositions specify change as simply linearly increasing or decreasing over time. Relate to repeated calls for time and lack of longitudinal research.   

(ref:preacher2015) [@preacher2015]
One way to develop specific recommendations is to model nonlinear patterns of change. In modelling nonlinear patterns of change, organizational research can develop a more nuanced understanding of change. As an example, using a structured latent growth curve model such as the one used in the current simulations to to model a logistic pattern of change, nonlinear change in a meaningful way. In the current simulations, the number of days needed to reach the halfway-  nd triquarter-halfway elevation points were estimated.\footnote{Note that parameters of nonlinear functions can be reparameterized to estimate other meaninfgul aspects of a curve (ref:preacher2015).} To add another level of meaning, a latent categorical variable can be added to the model to create a growth mixture model [@vandernest2020]. Using a growth mixture model, not only can nonlinear change be defined in a meaningful way, but response groups can be modelled and people can be categorized into groups based on their pattern of change. Thus, a researcher not only has an understanding of nonlinear patterns of change, but how frequently each pattern of change occurs. 


Given that change over time is likely to be nonlinear [@cudeck2007] and that few studies have modelled these nonlinear dynamics in organizational psychology, plenty of opportunities exist for researchers to document temporal dynamics. 

In modelling change, researchers can develop richer theory and, consequently, conduct stronger tests.  


### How Should Researchers Model Nonlinear Change Over Time? 

Use models whose parameters can be more practically interpretations.   plynomal vs nonlinear. 

Use latetn growth curve model. List benefits. Qualigy mlm.  Shy away from sing multilevel modelling because of more difficult optimization and also because of limitations of MLM. 

Using latent growth curve framework, either a nonlinear function can be directly used or bilinear spline models can be used. Note that spline models present the additional challenge of locating knot points. 

 
## Conclusion 

In systematically reviewing the simulation literature, I found that.. To address these gaps in the literature, I conducted three simulation experiments. 
