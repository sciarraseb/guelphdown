
```{r package_loading_int_conc, include=F}
#devtools::install_github(repo = 'sciarraseb/nonlinSimsAnalysis', force = T)
library(easypackages)
packages <- c('tidyverse', 'RColorBrewer', 'parallel', 'data.table', 'kableExtra', 'ggtext', 'egg', 'ggbrace', 'cowplot', 'nonlinSimsAnalysis', 'nonlinSims')
libraries(packages)

knitr::opts_chunk$set(message = F)
```


# General Discussion 

In systematically reviewing the simulation literature, I found that no study had comprehensively investigated the effects of longitudinal design and
analysis factors on the modelling accuracy of nonlinear patterns of change. Specifically, no study had investigated any possible three-way interaction between any of the following four variables: 1) measurement spacing, 2) number of measurements, 3) sample size, and 4) time structuredness. Given that longitudinal designs are necessary to understand the temporal dynamics of psychological processes (for a more detailed explanation, see Appendix \ref{ergodicity}), it is important that researchers understand how longitudinal design and analysis factors affect the accuracy of longitudinal analyses and if there are any interactions between these factors. Therefore, to address these gaps in the literature, I  designed three simulation experiments.

In each simulation experiment, a logistic pattern of change (i.e., s-shaped change pattern) was modelled under conditions that varied in nature of change, measurement number, sample size, and time structuredness.\footnote{Importantly, no simulation experiment manipulated more than three variables at once.} To fit a logistic function where each parameter could be meaningfully interpreted, each simulation experiment used a structured latent growth model to estimate nonlinear change (for a detailed explanation, see Appendix \ref{structured-lgc}). 

To investigate the effects of longitudinal design and analysis factors on modelling accuracy, my simulation experiments examined the accuracy with which each logistic function parameter was estimated. Thus, modelling accuracy was defined at the parameter level. Importantly, in giving modelling accuracy a parameter-level definition, it became important to measure two metrics: bias and precision. For any given logistic function parameter, two questions are of central importance: 1) How well is the parameter estimated on average (bias) and 2) what is a plausible range of values that can be expected for an estimate from the output of a single model (precision). Therefore, bias and precision were computed for the estimation of each logistic function parameter. To succinctly summarize each experiment, I have created Table \ref{tab:exp-summary-table}. Each row of Table \ref{tab:exp-summary-table} contains a summary of a simulation experiment.

In Experiment 1, I was interested in answering two questions: 1) Does placing measurements near periods of change increase modelling accuracy and 2) how should measurements be spaced when the nature of change is unknown. To answer these two questions, I manipulated measurement spacing, number of measurements, and nature of change. With respect to the first question, the results of Experiment 1 suggest that modelling accuracy increases when measurements are placed closer to periods fo change (see section discussing [measurement spacing](#meas-placing)). With respect to the second question, the results of Experiment 1 suggest that measurements should be spaced equally over time when the nature of change is unknown (see section discussing measurement spacing when the nature of change is [unknown](#unknown)). 

```{r exp-summary-table, echo=F}
summary_df <- data.frame('Simulation Exeriment' = c("Experiment 1", "Experiment 2", "Experiment 3"), 
                         'Independent Variables' = c("\\thead[lt]{Spacing of measurements \\\\ Number of measurements \\\\ Nature of change}", 
                                                     "\\thead[lt]{Spacing of measurements \\\\ Number of measurements \\\\ Sample size}", 
                                                     "\\thead[lt]{Number of Measurements \\\\ Sample size \\\\ Time structuredness}"), 
                         'Main Results' = c('\\tabitem Modelling accuracy is higher when measurements are placed closer to periods of change \\newline
                                            \\tabitem Measurements should be spaced equally when the nature of change is unknown',
                                            '\\tabitem The greatest improvements in modelling accuracy result from using either seven measurements with $N \\ge$ 200 or nine measurements with $N \\le$ 100',
                                            '\\tabitem The greatest improvements in modelling accuracy across all time structuredness levels result from using either seven measurements with $N \\ge$ 200 or nine measurements with $N \\le$ 100 \\newline
                                            \\tabitem Use definition variables to prevent modelling accuracy from decreasing as time structuredness decreases'), check.names = F)



kbl(x = summary_df, format = 'latex', digits = 2,align = c('l', 'l', 'l'), 
    longtable = T, booktabs = T,  escape = F,
    caption = 'Summary of Each Simulation Experiment') %>%
    column_spec(column = 3, width = '7.25cm') %>%
   kable_styling(latex_options= c('repeat_header'), position = 'left')

```

In Experiment 2, I was interested in the measurement number/sample size pairings needed to obtain accurate modelling (i.e., low bias, high precision) under different spacing schedules. To answer this question, I manipulated measurement spacing, measurement number, and sample size. Although no manipulated measurement number-sample size pairing results in high modelling accuracy (low bias, high precision) of all parameters, the largest improvements in modelling accuracy result from using moderate measurement numbers and sample sizes. For all spacing schedules (except middle-and-extreme spacing), the largest improvements in modelling accuracy result from using either either seven measurements with *N* $\ge$ 200 or nine measurements with *N* $\le$ 100.  The results for middle-and-extreme spacing are largely an effect of the nature of change used in Experiment 2, and so are of little value to emphasize. 

In Experiment 3, I was interested in examining how time structuredness affected modelling accuracy. To answer this question, I manipulated measurement spacing, measurement number, and time structuredness. Although the measurement number-sample size pairings that result in the greatest improvements in modelling accuracy are the same as in Experiment 2, two results suggest that modelling accuracy decreases as time structuredness decreases. First, precision decreases as time structuredness decreases. Second, and more concerning, bias decreases as time structuredness decreases regardless of the measurement number or sample size. Importantly, the decrease in modelling accuracy can be prevented by using a latent growth curve model with definition variables, which I showed in an additional set of simulations (see section on [definition variables](#def-variables)). Therefore, to obtain the largest improvements in modelling accuracy, either seven measurements with *N* $\ge$ 200 or nine measurements with *N* $\le$ 100 must be used and, importantly, the latent growth curve model must use definition variables. 

In summary, the results of my simulation experiments are the first (to my knowledge) to provide specific measurement number and sample size recommendations needed to accurately model nonlinear change over time. Importantly, although previous studies have investigated the effects of some longitudinal design and analysis factors on the modelling accuracy of nonlinear patterns, the results of these studies are limited because they either used unrealistic fixed-effects models [e.g., @finch2017], models with non-meaningful parameter interpretations [e.g., @fine2019; @liu2022], or unrealistic model fitting procedures [@finch2017]. Additionally, I developed novel and replicable procedures for creating spacing schedules (see Appendix \ref{measurement-schedules}) and simulating time structuredness (see [time structuredness](#simulating-time-struc)). 

The sections that follow will discuss....

## Limitations and Future Directions 

### Cutoff Values for Modelling Accuracy

In simulation research, cutoff values for parameters are often set to a percentage of a parameter's population value [e.g., @muthen1997] for two reasons. First, cutoff values are needed to allow the biasness and precision of modelling performance to be categorized so that results can be clearly presented. In the current set of simulation experiments, cutoff values for bias and precision were set to 10% of the parameter's population value [@muthen1997]. If a parameter estimate lied outside a 10%-error margin, then estimation was considered biased. If an error bar whisker length was longer than 10% of the parameter's population value, then estimation was considered imprecise. Therefore, using cutoff values allows categorical decisions to be made modelling performance. 

Second, cutoff values are needed to allow results from different simulation studies to be meaningfully compared. If another study uses a cutoff value of 15%, then the results of this comparison cannot be validly compared to the results of the current simulation experiments because each study uses different standards. Therefore, it is important that simulation studies use a common standard of 10% [@muthen1997]---as I have done in my simulation experiments. Although simulation studies use cutoff values to simplify results and allow meaningful comparisons of results, it is also important that cutoff values themselves represent meaningful boundary values.  

In simply defining cutoff values as a percentage of a population value, cutoff values can become meaningless and lead to problematic decision making. As a simple example, consider a scenario where a beverage company wants to produce a caffeinated drink that can only increase heart rate and body temperature by a certain amount. Specifically, neither heart rate nor body temperature can increase by 10% of their resting values. Given that, for males and females, any value below 70 and 80, respectively, constitutes a healthy resting heart rate [@nanchen2018], a 10% increase would translate to an increase of 7 and 8 beats per minute, which is arguably less than the increase in heart rate caused from walking [e.g., @whitley1987]. Thus, requiring that a caffeinated drink not increase resting heart rate by a value equal to or greater than 10% appears to be a responsible stipulation. Unfortunately, setting a 10%-cutoff rule for body temperature allows far less desirable outcomes than a 10% cutoff for heart rate. Using a typical body temperature of 37 $^\circ$C for resting body temperature, a 10%-cutoff would allow for a change in body temperature of 3.7 $^\circ$C. Given that deviations of less than 3.7 $^\circ$C from resting body temperature can lead to physiological impairments and even death [@moran2002], restricting the caffeinated drink to not increase body temperature by 10% of its resting value is unwise. Therefore, a percentage cutoff rule can fail to create meaningful cutoff values by overlooking the underlying nature of the corresponding variable.

In the current simulation experiments, the percentage-cutoff rule may have led to incorrect decisions about modelling performance. As an example, consider the estimation of the random effect parameters. In each simulation experiment, no measurement number/sample size pairing resulted in high modelling accuracy (low bias, high precision) of any random-effect parameter. Specifically, the random-effect day-unit parameters were never modelled precisely with any measurement number/sample size pairing.\footnote{Note that not even any of the random-effect Likert-unit parameters were ever modelled with high precision by any manipulated measurement number/sample size pairing.} Although the lack of accurate estimation for all parameters seems concerning, the result be a byproduct of how cutoff values were set to deem estimation as precise. For a given parameter, the cutoff value used to deem estimation as unbiased and precise was proportional to the population value set for that parameter. Specifically, the cutoff values for bias and precision were set to 10% of the parameter's population value [@muthen1997]. In setting the cutoff value to a percentage of the parameter's population value, the margin of error becomes a function of the population value: Large population values have large margins of error and small population values have small margins of error. Given that the random-effect parameters had the smallest population values (e.g., 10.00, 4.00, and 0.05) and that even the largest measurement number-sample size pairing of 11 measurements with *N* = 1000 did not model with high precision, it is conceivable that the associated 10%-error margins (e.g., 1.00, 0.04, and 0.005) may have been too small. 

Because percentage cutoff rules are relative to the population value set for a parameter, they can often be arbitrary. One way to ensure less arbitrary cutoff values is to conceptualize cutoff values as smallest effect sizes of interest [@lakens2017; @lakens2018]. Introduced to improve to null-hypothesis significance testing, a smallest effect size of interest constitutes the smallest effect size above which a researcher considers an observed effect meaningful [@lakens2017]. Instead of testing the typical zero-effect null hypothesis, a researcher can specify a smallest effect size of interest as the null hypothesis. Using a smallest effect size of interest (in tandem with equivalence testing), a researcher can more definitely conclude whether an effect is trivially small or not and, consequently, be less likely to incorrectly dismiss an effect as nonexistent. Thus, smallest effect sizes of interest allow researchers to make more meaningful conclusions. Although the current simulation experiments did not employ significance testing, the cutoff values used to determine whether estimation was biased and precise could be improved by treating them as smallest cutoff values of interest. By replacing the current percentage-based cutoff values with smallest cutoff values of interest for each parameter, conclusions are likely to become more meaningful. 

One effective way to determine smallest cutoff values of interest is to use anchor-based methods [@anvari2021]. As an example, I detail a two-step procedure for how an anchor-based method could be used to determine a cutoff value for a the Likert-unit parameter of the fixed-effect baseline parameter ($\uptheta_{fixed}$). First, a survey for some Likert-unit variable such as job satisfaction could be given at two time points to employees. Importantly, after completing the survey at the second time point, employees would also indicate how much job satisfaction changed by answering an anchor question (e.g., "Job satisfaction increased/decreased by a little, increased/decreased a lot, or did not change."). Second, a smallest cutoff value of interest would need to be computed at two time points. Given that the fixed-effect baseline parameter ($\uptheta_{fixed}$) represents the starting value, then employees that indicated no change in job satisfaction could be said to still be at baseline. Thus, to compute a smallest cutoff value of interest for the fixed-effect baseline parameter ($\uptheta_{fixed}$), the mean change in job satisfaction could be computed using data from employees that indicated no change. Therefore, in using the anchor-based method, the smallest cutoff value of interest for the fixed-effect baseline parameter ($\uptheta_{fixed}$) is the mean change in some Likert-unit variable---job satisfaction in the current example---from respondents that indicated no change.\footnote{If the mean observed change in job satisfaction from employees that indicate no change is a near-zero value, using this value as a smallest effect-size of interest for the fixed-effect baseline parameter ($\uptheta_{fixed}$) would likely be too conservative. In such situations, the smallest effect-size of interest for the fixed-effect baseline parameter ($\uptheta_{fixed}$) could be determined by computing the mean change in job satisfaction from employees that indicated a small change (i.e., 'little increase/decrease), as it could be said that these employees have slightly moved away from baseline.}

In summary, percentage-based cutoff values are used in simulation research to allow categorical decisions to be made about modelling performance and to enable results to be meaningfully compared between studies. Unfortunately, in defining cutoff values as a percentage of a population value, cutoff values can become meaningless and lead to problematic decision making. To set cutoff values that specify meaningful boundaries, cutoff values can be conceptualized as smallest effect sizes of interest. In empirical research, a smallest effect size of interest constitutes the smallest effect size above which a researcher considers an observed effect meaningful [@lakens2017]. By using smallest effect sizes of interest, a researcher can more definitely conclude whether an effect is trivially small or not and, consequently, be less likely to incorrectly dismiss an effect as nonexistent. Similar to empirical research, smallest cutoff values of interest can be used in simulation research so that so that quantitative researchers can more definitely identify conditions that produce meaningful amounts of error. To determine smallest cutoff values of interest, anchor-based methods [@anvari2021] can be used. 


### External Validity of Simulation Experiments

In the current set of simulation experiments, data were were generated under ideal conditions in five ways. First, the current simulation experiments always assumed complete data (i.e., 100% response rate). Unfortunately, researchers rarely obtain complete data and, instead, have some amount of data that are missing. One investigation estimated that, using a sample of 300 articles published over a period of three years, 90% of articles had missing data, with each study estimated to have over 30% of data points missing [@mmcknight2007, Chapter 1]. Perhaps even more concerning, missing data often compound over time [@newman2003].\footnote{It should be noted that great advice exists on increasing response rate. In fact, an entire book of advice exists on this issue [see @dillman2014].} To simulate more realistic conditions for response rates in longitudinal designs, missing data could be set to increase---either linearly or nonlinearly---over time under three types of commonly simulated missing data mechanisms: 1) missing data are random, 2) missing data depend on the value of another variable, and 3)  missing data depend on their own values [@newman2004]. 

Second, the current simulation experiments assumed measurement invariance over time. That is, at each time point, the manifest variable is measured with the same measurement model [@mellenbergh1989]---specifically, aspects of the measurement model such as factor loadings, intercepts, and error variances remain constant over time [@vandenberg2000]. For a longitudinal design, it is important that the measurement of a latent variable meet the conditions for invariance so that change over time can be meaningfully interpreted. As an example, consider a situation where a researcher measures some latent variable over time such as job satisfaction using a four-item survey where each item measures some component of job satisfaction on a Likert scale (range of 1--5). If the loadings of a specific item change over time, then the response values from participants cannot be meaningfully interpreted. For example, if a participant gives the same answers to each item across two time points but factor loadings of any item(s) change between the two time points, then their job satisfaction scores between the time points will, counterintuitively, be different. Thus, even though job satisfaction did not change over time, changes in the measurement model of job satisfaction caused the observed scores to be different. Unfortunately, measurement invariance is seldom observed [@vandeschoot2015] because measurement model components often change over time [e.g., @fried2016]. Thus, it can be argued that it is more realistic to assume measurement noninvariance. To simulate measurement noninvariance, data could be generated such that aspects of the measurement model change are set to change over time [e.g., @kim2014]. 

Third, the current simulations assumed error variances in the observed variables to be constant and uncorrelated over time. Unfortunately, error variances over time are likely to correlate with each other and be nonconstant or heterogeneous [@deshon1998; @bliese2002; @ding2016; @lester2019; @blozis2018]. To simulate a more realistic error variance structure, simulations could simulate errors to correlate with each other and to decrease over time---an observation in a longitudinal analysis of fatigue [@lang2018]. 

Fourth, the current simulations investigated the effects of longitudinal design and analysis factors with a simple model. In addition to the No covariances set between logistic function parameters

In summary, ... Given that each factor itself decreases model performance, it is likely that decrease as simulation conditions become more realistic. 

@sivo2000; @sivo2005; @sivo2008; @grimm2010a]

Third, the current simulation experiments assumed perfect measurement of the latent growth factors. Given that many variables in organizational psychology are unobservable---that is, latent variables (e.g., job satisfaction, personality traits, trust, etc.)---they cannot be directly measured, and so must be approximated by developing representations or constructs. Because latent variables then are approximations of psychological variables, the measurement of these variable will inevitably contain error. Thus, many variables in psychology contain error. In a longitudinal setting, the 

### Simulations With Other Longitudinal Analyses

Second-order latent growth models
Growth mixture models 
Spline models 
Autoregressive latent trajectory models 
Time series analysis


## Nonlinear Patterns and Longitudinal Research

Because change is nonlinear ---> need maby more moeasurements 

Longitudinal research should investigate temporal dynamics over time. Given that change over time is unlikely to be linear, then pre-post studies should not be considered longitudinal and should simply be left as pre-post research. Although some definitions of longitudinal research have suggested a minimum of three measurements [@ployhart2010], the results of all three simulation experiments suggest that, for modelling even a simple nonlinear change pattern, at least seven measurements are needed. 


### Why is it Important to Model Nonlinear Patterns of Change? 

For at least 30 years, research in organizational psychology has had a minimal effect on the implementations of practitioners [@daft1990; for a review, see @lawler2022]. Almost no practitioner--specifically, an estimated 1%---reads journal articles [@ryne2002], which is accompanied by a poor understanding of fundamental organizational psychology principles by managers across multiple cultures including the Netherlands [@sanders2008], the United States [@ryne2002], Finland, South Korea, and Spain [@tenhiälä2014]. Perhaps most unfortunate, an estimated 55% of practitioners are skeptical that evidence-based human resource practices can affect any positive change [@kpmg2015]. With the gap between academics and practitioners being so patently wide, some academics have cast doubt on the possibility of academic- practitioner research collaborations [@kieser2009].  

One factor that likely contributes to the academic-practitioner gap is the paucity of specific recommendations. Comprehensive investigations of the organizational literature have found that an estimated 3% of human resources articles address the problems of practitioners [@sackett1990] and, in a review of 5780 articles from 1963--2007, it was concluded that research is often late to address practitioner issues [@cascio2008].   

One way to develop specific recommendations is to model nonlinear patterns of change. In modelling nonlinear patterns of change, organizational research can develop a more nuanced understanding of change. As an example, consider a situation where two researchers model change. The first researcher models the data with an intercept-slope model and the second researchers uses a logistic function to model change. In modelling change as linear.


Given that change over time is likely to be nonlinear [@cudeck2007] and that few studies have modelled these nonlinear dynamics in organizational psychology, plenty of opportunities exist for researchers to document temporal dynamics. \footnote{As an important aside, researchers should also pre-register studies so avoid the many *p*-hacking to avoid the many deleterious effects of non-registered research} 


In modelling change, researchers can develop richer theory and, consequently, conduct stronger tests.  


### How Should Researchers Model Nonlinear Change Over Time? 

Use models whose parameters can be more practical interpretations.   plynomal vs nonlinear. 

Use latetn growth curve model. List benefits. Qualigy mlm.  Shy away from uinge multilevel modelling because of more difficult optimization and also because of limitations of MLM. 

Using latent growth curve framework, either a nonlinear function can be directly used or bilinear spline models can be used. Note that spline models present the additional challenge of locating knot points. 



## Conclusion 


